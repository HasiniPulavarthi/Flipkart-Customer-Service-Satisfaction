{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1l1HRBYLo7SXnt3JRzo7jtW9Jbr9fPHNn","timestamp":1771944702834}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","yQaldy8SH6Dl","PH-0ReGfmX4f","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - Flipkart Customer Service Satisfaction\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Classification\n","##### **Contribution**    - Individual\n","##### **Team Member 1 -** Hasini Pulavarthi\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["Write the summary here within 500-600 words."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["**Objective:**"],"metadata":{"id":"rUh6nbdbLLHX"}},{"cell_type":"markdown","source":["The aim of this project is to develop a classification model to predict customer satisfaction scores for Flipkart. This predictive model will help the company identify areas for improvement in customer experience and optimize their marketing and service strategies, ultimately enhancing customer retention and satisfaction."],"metadata":{"id":"t3RLyBkwJEVi"}},{"cell_type":"markdown","source":["**Dataset Overview:**"],"metadata":{"id":"_A9tg9TlLOuF"}},{"cell_type":"markdown","source":["\n","The dataset comprises 20 columns, with several features showing a strong correlation to customer behavior. Key customer-related variables includes Order_ID, Order_date_time, Customer_City, Item_Price, Channel_Name, and Customer_Remarks. Additionally, key operational variables include Issue_Responded_at, Issue_Responded, Survey_Response_Date, Product_Category, Agent_Name, Supervisor, Manager, Agent_Shift, and Sub_Category. The target variable is the CSAT Score, which ranges from 1 to 5, indicating customer satisfaction—higher scores reflect greater satisfaction."],"metadata":{"id":"Fay81k3gJdfd"}},{"cell_type":"markdown","source":["**Data Manipulation:**"],"metadata":{"id":"A8mKmxaiLfaC"}},{"cell_type":"markdown","source":["\n","The date variables were converted from object type to datetime format for improved processing. The Connected_Handling_Time column was removed due to having 99% of its values as null. Additionally, any rows with null values in the Order_ID column were eliminated. To address missing data, null values in categorical columns, including Customer_City, Customer_Remarks, and Product_Category, as well as in numeric columns like Item_Price, Issue_Reported_at, and Issue_Responded, were filled appropriately."],"metadata":{"id":"56CSvm4rJuYs"}},{"cell_type":"markdown","source":["**Exploratory Data Analysis****(EDA)** **:**"],"metadata":{"id":"hJWROsXgLlMF"}},{"cell_type":"markdown","source":["The EDA phase involved a comprehensive exploration of the dataset to uncover trends, patterns and relationships that could inform the model-building process."],"metadata":{"id":"4JOEeJkAKOXg"}},{"cell_type":"markdown","source":["**Data cleaning and Preparation:**"],"metadata":{"id":"PRFjQQ1wMIA1"}},{"cell_type":"markdown","source":["Handling Missing Data: The dataset was evaluated for missing values, and suitable imputation strategies were implemented to maintain data integrity."],"metadata":{"id":"3JR2DumPKTuN"}},{"cell_type":"markdown","source":["**Feature Engineering:**"],"metadata":{"id":"avLJZJVWMYpg"}},{"cell_type":"markdown","source":["Data Types: The year, month, day, hour, and minute were extracted from the Order_Date_Time column to derive additional valuable insights. Outliers: The Item_Price column was analyzed for outliers using the IQR method, leading to the removal of 23.85% of data points that fell outside acceptable limits. Encoding: One-Hot Encoding was applied to categorical features such as Channel_name and Product_Category."],"metadata":{"id":"_SXcgFH2KX1L"}},{"cell_type":"markdown","source":["**Univariate Analysis:**"],"metadata":{"id":"tRoHlXcJM4gi"}},{"cell_type":"markdown","source":["Histograms and box plots were employed to analyze the distribution of continuous variables like Item_price and CSAT Score."],"metadata":{"id":"4e3Yoi9YKdXA"}},{"cell_type":"markdown","source":["**Data Splitting:**"],"metadata":{"id":"HFaEqD06LEjH"}},{"cell_type":"markdown","source":["The data was split into training and test sets with an 80-20 ratio. Several classification models were trained, including Logistic Regression, Decision Trees, Random Forest, kNeighbors and Extratrees classifier. Hyperparameter tuning was performed, focusing on F1-Score as the primary evaluation metric to balance precision and recall."],"metadata":{"id":"gxdevkbCKiff"}},{"cell_type":"markdown","source":["**Model Evaluation:**"],"metadata":{"id":"dOFBBxtpK9LH"}},{"cell_type":"markdown","source":["The models were evaluated using Precision, Recall, F1-Score, and Accuracy. The Extra trees model exhibited the best performance, particularly in terms of precision and accuracy, making it the model of choice. The confusion matrix and ROC-AUC curve further supported the model's effectiveness in distinguishing between satisfied and dissatisfied customers. Feature Importance:\n","\n","The model identified Customer_Remarks, issue_responded, Channel_name and category as the most significant factors influencing customer satisfaction. This suggests that Customer_Remarks, issue_resoponded are crucial for enhancing customer satisfaction."],"metadata":{"id":"gey1QMfxKoHz"}},{"cell_type":"markdown","source":["**Conclusion:**"],"metadata":{"id":"PNOB9kQtK3o9"}},{"cell_type":"markdown","source":["The final model successfully predicts customer satisfaction scores with high precision. This model can be deployed to enhance Flipkart's customer experience initiatives, ensuring that resources are allocated to customers with the highest potential for satisfaction. Insights from feature importance analysis can guide strategic decisions in marketing and service improvements."],"metadata":{"id":"-XaGSzWWKt02"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["https://github.com/HasiniPulavarthi/Flipkart-Customer-Service-Satisfaction.git"],"metadata":{"id":"EueHV83Zz6BY"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["Problem Statement: Predicting Flipkart Customer Satisfaction Scores Using Predictive Analytics and Machine Learning"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["#### **Define Your Business Objective?**"],"metadata":{"id":"PH-0ReGfmX4f"}},{"cell_type":"markdown","source":["**Context:**"],"metadata":{"id":"1fjeB8IkNmNp"}},{"cell_type":"markdown","source":["In the highly competitive e-commerce industry, customer satisfaction is a critical determinant of brand loyalty and long-term growth. As one of the largest e-commerce platforms, Flipkart handles millions of customer interactions across various support channels such as email, phone, live chat, and social media. Maintaining high levels of customer satisfaction is essential for retaining customers and enhancing their shopping experience. Understanding the factors that influence customer satisfaction and being able to predict CSAT scores based on these factors can significantly improve Flipkart's customer service performance and help in optimizing resources."],"metadata":{"id":"eX_oTpabNrJ-"}},{"cell_type":"markdown","source":["**Objective:**"],"metadata":{"id":"LGpKiy6_Nts0"}},{"cell_type":"markdown","source":["The primary objective of this project is to develop a machine learning model that can predict customer satisfaction scores (CSAT) based on historical customer interaction data. By analyzing different features such as customer demographics, interaction types, issue resolution times, support channel performance, and agent efficiency, the model will identify the key drivers of customer satisfaction and provide actionable insights for improving the overall customer service experience. The target variable is the CSAT score, which reflects the satisfaction level of customers after interacting with Flipkart's support team."],"metadata":{"id":"52Wle91xNxxw"}},{"cell_type":"markdown","source":["**Key Challenges:**"],"metadata":{"id":"8slc4n64N0Eg"}},{"cell_type":"markdown","source":["Data Imbalance: The dataset may contain an imbalanced distribution of high and low satisfaction scores, which can lead to biased predictions. Addressing this issue with techniques like oversampling, undersampling, or advanced algorithms will be crucial.\n","\n","Feature Engineering: Accurately capturing the drivers of customer satisfaction will require careful feature engineering. This may involve creating new features related to agent performance, interaction duration, issue complexity, and support channel usage to improve model accuracy.\n","\n","Multichannel Integration: Customer interactions occur across different support channels (e.g. inbound, outcall, email). Integrating and normalizing these interactions for model input poses a challenge, especially when data comes in different formats and volumes from each channel.\n","\n","Scalability: Given the high volume of customer interactions, the model needs to be scalable to handle large datasets in real-time scenarios while maintaining prediction accuracy.\n","\n","Model Evaluation: Since the goal is to accurately predict satisfaction levels, precision, recall, and F1-score metrics will be essential in evaluating the model’s performance, with a particular focus on avoiding false negatives (i.e., predicting high satisfaction when the customer is actually dissatisfied)."],"metadata":{"id":"nIgqEgRPN3ep"}},{"cell_type":"markdown","source":["**Deliverables:**"],"metadata":{"id":"bYOI2Q0fN5oR"}},{"cell_type":"markdown","source":["A machine learning model capable of predicting customer satisfaction (CSAT) based on customer interaction data.\n","\n","A detailed report on data preprocessing steps, feature engineering, model selection, evaluation metrics, and key insights derived from the analysis."],"metadata":{"id":"1IZ45TGUN8e7"}},{"cell_type":"markdown","source":["**Business Impact:**"],"metadata":{"id":"XtJ4lfu2N-xy"}},{"cell_type":"markdown","source":["By accurately predicting customer satisfaction, Flipkart can prioritize high-risk customers and take proactive measures to resolve potential issues before they escalate. The insights generated from the model will allow Flipkart to optimize its customer support processes, such as allocating more resources to underperforming channels, rewarding high-performing agents, and customizing support strategies for different customer segments. Ultimately, improving CSAT scores will enhance brand loyalty, reduce customer churn, and drive higher customer retention rates, contributing to Flipkart’s long-term success in the competitive e-commerce market."],"metadata":{"id":"_Zc7LsL6OBet"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"PhDvGCAqmjP1"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 20 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Importing Data Framework Libraries.\n","import pandas as pd\n","import numpy as np\n","\n","# Importing warnings.\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Visualisation libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Data Preprocessing libraries\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# Train Test Split\n","from sklearn.model_selection import train_test_split\n","\n","# ML models libraries\n","from sklearn.linear_model import LinearRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","\n","# Evaluation\n","from sklearn.metrics import accuracy_score, recall_score, classification_report, precision_score, f1_score, ConfusionMatrixDisplay"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load the dataset and handle the errors\n","try:\n","  #load the data set\n","  df = pd.read_csv('/content/Customer_support_data.csv')\n","  print('Dataset loaded succesfully')\n","except FileNotFoundError:\n","  print('Dataset not found')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","df.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","print(f\"Number of Rows: {df.shape[0]}\")\n","print(f\"Number of Columns: {df.shape[1]}\")"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","print(f\"Number of duplicate values: {df.duplicated().sum()}\")"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","df.isnull().sum()"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Percentage of null values\n","missing_percentage = df.isnull().mean() * 100\n","print(missing_percentage)"],"metadata":{"id":"Y7_25eBJRhn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(df.isnull(), cbar=False)"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["\n","\n","1.   Customer_support_data contains 85907 Rows and 20 Columns.\n","2.   The Data set has different types of data type including (int, object, float).\n","3.  Seven Columns in the Data set has null values.\n","4.  Sum of nullvalues of the overall Data set is 4,35995, which is very high.\n","5.  By plotting the heatmap it is observed that 20 columns of the data set there are 7 columns which has null values and the rest of the columns has 0 null values.\n","\n"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","df.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","df.describe()"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"vzWSa4pLSiDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["1. Unique_id: Unique identifier for each entry or record in the dataset\n","2. channel_name: Name of the channel where the issue or feedback was reported.\n","3. category: Category of the issue or feedback.\n","4. Sub-category: Specific sub-category under the main category.\n","5. Customer Remarks: Comments or remarks provided by the customer.\n","6. Order_id: Unique identifier for each order.\n","7. order_date_time: Date and time when the order was placed.\n","8. Issue_reported at: Timestamp indicating when the issue was reported.\n","9. issue_responded: Timestamp indicating when the issue was responded to.\n","10. Survey_response_Date: Date when the customer survey response was recorded.\n","11. Customer_City: City of the customer.\n","12. Product_category: Category of the product related to the issue or feedback.\n","\n","13. Item_price: Price of the item associated with the order.\n","\n","14. connected_handling_time: Time taken to handle the connection or issue.\n","\n","15. Agent_name: Name of the agent who handled the issue or feedback.\n","\n","16. Supervisor: Name of the supervisor overseeing the issue or feedback.\n","\n","17. Manager: Name of the manager overseeing the process.\n","\n","18. Tenure Bucket: Bucket representing the tenure or duration related to the agent or issue.\n","\n","19. Agent Shift: Agent shift indicates the shift type based on (Morning,Evening, Afternoon, Night, Split)\n","\n","20. Csat score: Csat score describes the satisfaction score for each order_id"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for i in df.columns.tolist():\n","  print(f\"No. of unique values in {i} is {df[i].nunique()}.\")"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# Before Doing any manipulation and Data wrangling task on data set we will make a copy of the original data set\n","fk_df = df.copy()"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the new data set after creating a copy.\n","fk_df.head()"],"metadata":{"id":"AoigIAhlUGjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fk_df.info()"],"metadata":{"id":"Wfa5GWIYULAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Since the connected handling time colum has 99 % of null values we will drop the whole column.\n","fk_df = fk_df.drop('connected_handling_time', axis = 1)"],"metadata":{"id":"-qkaM45-UUYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing null rows from Order_id Column.\n","fk_df = fk_df.dropna(subset=['Order_id'])"],"metadata":{"id":"FfLVdSaWUaTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting Order_date_time Columnn from object to datetime format.\n","fk_df['order_date_time'] = pd.to_datetime(fk_df['order_date_time'],format='%d/%m/%Y %H:%M', errors = 'coerce')"],"metadata":{"id":"Zo74tZ7DUd2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting Issue_reported_at Column from object to datetime format.\n","fk_df['Issue_reported at'] = pd.to_datetime(fk_df['Issue_reported at'],format ='%d/%m/%Y %H:%M', errors = 'coerce')"],"metadata":{"id":"rgeRCwQLUjM7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting issue_responded Column from object to datetime format.\n","fk_df['issue_responded'] = pd.to_datetime(fk_df['issue_responded'], format = '%d/%m/%Y %H:%M', errors = 'coerce')"],"metadata":{"id":"GJWMz5EoUoaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting Survey_response_Date Column from object to datetime format.\n","fk_df['Survey_response_Date'] = pd.to_datetime(df['Survey_response_Date'], format='%d-%b-%y', errors='coerce')"],"metadata":{"id":"IVw6JaavUvcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sum of Null values of Customer Remarks column.\n","fk_df['Customer Remarks'].isnull().sum()\n","print(f\"Sum of Null values of Customer Remarks Column: {fk_df['Customer Remarks'].isnull().sum()}\")"],"metadata":{"id":"xPcHkZZBU2jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Since order_date_time has 68693 Null values we will group by the column with Survey response Date to fill the Null values.\n","fk_df['order_date_time'] = fk_df.groupby('Survey_response_Date')['order_date_time'].transform(lambda x: x.fillna(x.median()))"],"metadata":{"id":"KUiN5MoHbmKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting various date and time components from the 'order_date_time' column\n","fk_df['day'] = fk_df['order_date_time'].dt.day_name()\n","fk_df['year'] = fk_df['order_date_time'].apply(lambda x: x.year)\n","fk_df['month_num'] = fk_df['order_date_time'].apply(lambda x: x.month)\n","fk_df['day_num'] = fk_df['order_date_time'].apply(lambda x: x.day)\n","fk_df['hour'] = fk_df['order_date_time'].apply(lambda x: x.hour)\n","fk_df['minute'] = fk_df['order_date_time'].apply(lambda x: x.minute)\n","fk_df['month'] = fk_df['order_date_time'].dt.month_name()"],"metadata":{"id":"vUba8XcNbogy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Percentage of null values in the Customer_city Column\n","missing_count = fk_df['Customer_City'].isna().sum()\n","total_count = len(fk_df)\n","missing_percentage = (missing_count / total_count) * 100"],"metadata":{"id":"QShDU-dqbto7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Percentage null values of customer city column is: {round(missing_percentage,2)} %\")"],"metadata":{"id":"yJ3dqeeVbwcl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fk_df['Customer_City'].isnull().sum()\n","print(f\"Sum of Null values of Customer_City Column: {fk_df['Customer_City'].isnull().sum()}\")"],"metadata":{"id":"r6a4UsIkbzQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filling Null values of Item_price Column.\n","fk_df['Item_price'].fillna(0, inplace = True)"],"metadata":{"id":"2NPFzbe0eKfx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["1. I first created a copy of the original dataset named fk_df for data wrangling and manipulation tasks.\n","2. After creating the copy, I visualized columns with null values by plotting a heatmap.\n","3. The connected_handling_time column had 99% missing values, so I dropped the entire column.\n","4. The order_date_time column had 50,461 missing values. I applied median imputation to fill these missing values, grouping by the Survey_response_date column. This method helps ensure more accurate imputations based on the grouping.\n","5. To gain additional insights from the dataset, I extracted the day, year, month number, day number, hour, minute, and month name from the order_date_time column.\n","6. The order_id column contained null values, and I removed the rows with missing data.\n","7. I converted the data types of columns such as order_date_time, Issue_reported at, issue_responded, and Survey_response_Date from object to datetime format.\n","8. For columns with null values, I filled them with placeholders.\n","9. I replaced null values in the Item_price column with 0.\n","10. The Customer_Remarks column had duplicate values for remarks like \"Good,\" so I stripped the variables to handle these duplicates."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Set default behavior to not show grid lines for all plots\n","plt.rcParams['axes.grid'] = False  # Disable grid lines globally"],"metadata":{"id":"MZNKRfhRep85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chart - 1 visualization code\n","# Filtering out rows where Product_category is 'Not Available'\n","filtered_fk_df = fk_df[fk_df['Product_category'] != 'Not Available']\n","\n","average_price = filtered_fk_df.groupby('Product_category')['Item_price'].mean().sort_values(ascending=False)\n","\n","# Figure size\n","plt.figure(figsize=(13, 5))\n","\n","colors = sns.color_palette(\"husl\", len(average_price))\n","average_price.plot(kind='bar', color=colors)\n","\n","plt.title('Average Item Price by Product Category', fontsize=16)\n","plt.xlabel('Product Category', fontsize=14)\n","plt.ylabel('Average Item Price', fontsize=14)\n","plt.xticks(rotation=45, ha='right')\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["I selected the bar chart to display the average item price across different product categories."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["The chart shows that the average price for mobiles is ₹23,000, while home appliances average ₹12,000. Furniture averages ₹8,000. Mobiles have the highest average price, followed by home appliances, with furniture in third place."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["The insights highlight that higher-value categories like mobiles and home appliances contribute significantly to revenue, aiding targeted marketing and sales strategies. However, the lower average price for furniture could indicate potential issues with profitability or consumer interest. Addressing this through promotions or improved offerings could enhance growth in that category."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# Calculate value counts and the most frequent channel\n","channel_counts = fk_df['channel_name'].value_counts()\n","most_frequent_channel = fk_df['channel_name'].mode()[0]\n","\n","explode = [0.1 if label == most_frequent_channel else 0 for label in channel_counts.index]\n","\n","plt.figure(figsize=(10, 6))\n","plt.pie(channel_counts, labels=channel_counts.index, autopct='%1.1f%%', explode=explode, startangle=140)\n","plt.title('Distribution of Channel Names')\n","plt.show()"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["I selected the pie chart to illustrate the distribution of various communication channels."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["By plotting the pie chart, I discovered that Inbound accounts for approximately 78.7% of the distribution, followed by Outcall at 17.4%, while Email has the smallest share at 3.8%."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["The insights gained can significantly benefit the business by identifying the primary channels where most customer interactions take place, such as Inbound and Outcall. This allows the company to allocate resources more effectively to these high-traffic channels, enhancing operational efficiency and customer experience. Conversely, the lower preference for the email channel indicates a potential area of concern, as it may negatively impact the business by not meeting customer expectations in this medium."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","category_counts = fk_df['category'].value_counts()\n","\n","plt.figure(figsize=(10, 6))\n","\n","# Plotting a histogram (bar chart) to show the count of each category\n","plt.bar(category_counts.index, category_counts, color=plt.cm.Set2.colors)\n","\n","plt.xlabel('Category')\n","plt.ylabel('Count')\n","plt.title('Distribution of Categories')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["I chose a bar chart to display the distribution of the different categories."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["The bar chart indicates that the \"Return\" category has the highest count, with more than 30,000 occurrences. The \"Order-related\" category follows with over 15,000, while the \"Refund-related\" category has around 5,000."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Answer Here The insights can drive positive business impact by highlighting areas for improvement, like addressing high return rates to boost customer satisfaction. However, the high number of returns and order-related issues could indicate problems with product quality or fulfillment processes, potentially leading to customer dissatisfaction and revenue loss if not resolved."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","category_sub_category_distribution = fk_df.groupby('category')['Sub-category'].value_counts().nlargest(10)\n","\n","category_sub_category_distribution.unstack().plot(kind='bar', stacked=False, figsize=(22, 10))\n","\n","plt.xlabel('Category', fontsize=18)\n","plt.ylabel('Count', fontsize=18)\n","plt.title('Distribution of Sub-categories by Category', fontsize=22)\n","plt.xticks(rotation=45, fontsize=14)\n","plt.yticks(fontsize=14)\n","plt.legend(title='Sub-category', fontsize=14, title_fontsize=16)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["I chose a bar chart to display the counts of the top 10 sub-categories, grouped by their respective categories. This visualization effectively highlights the frequency of each sub-category within its category, providing a clearer view of their distribution across different categories."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["The bar chart illustrates the distribution of sub-categories within each main category, highlighting that \"Returns\" has the highest frequency, indicating a major area of concern. This is followed by \"Reverse Pickup Enquiry\" and \"Return Request,\" suggesting significant customer interaction in these areas as well. Understanding these patterns can help the business prioritize resources and develop targeted strategies to address common issues, ultimately improving customer satisfaction and operational efficiency."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["Yes, focusing on the \"Returns\" category and high counts for \"Reverse Pickup Enquiry\" and \"Return Request\" allows the business to address key areas, improve customer satisfaction, and reduce operational costs.\n","\n","High return volumes could indicate issues with product quality or customer dissatisfaction, potentially increasing costs and straining resources. Addressing these problems is crucial to avoid negative growth."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","customer_remarks = fk_df['Customer Remarks'].value_counts()\n","# Filtering out 'na' values\n","customer_remarks = customer_remarks[customer_remarks.index != 'na']\n","\n","largest_remarks = customer_remarks.nlargest(10)\n","\n","# Generate colors for each bar\n","largest_colors = plt.cm.get_cmap('tab10', len(largest_remarks)).colors\n","\n","# Plotting\n","plt.figure(figsize=(12, 6))\n","plt.bar(largest_remarks.index, largest_remarks, color=largest_colors, alpha=0.7, label='Count')\n","plt.plot(largest_remarks.index, largest_remarks, marker='.', color='b', label='Trend')\n","plt.xlabel('Customer Remarks', fontsize=14)\n","plt.ylabel('Count', fontsize=14)\n","plt.title('Distribution of Largest Customer Remarks', fontsize=16)\n","plt.xticks(rotation=45, fontsize=12)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["I have picked vertical bar chart to identify the count of customer remarks."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["The chart indicates that \"good\" is the most common remark, with 2,000 occurrences, while \"very good\" has over 500 instances. This suggests an opportunity to enhance the feedback process, potentially encouraging more customers to share their experiences."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["Positive Business Impact:"],"metadata":{"id":"z_pnpToXgvtg"}},{"cell_type":"markdown","source":["The chart indicates that a large portion of customer feedback is positive, which can be used to showcase strengths and enhance customer satisfaction."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["Potential for Negative Growth:"],"metadata":{"id":"Kn3KdK9Jg1am"}},{"cell_type":"markdown","source":["The less frequent remarks, such as \"good service\" and \"excellent,\" suggest that many customers may not be fully satisfied, potentially indicating a lack of engagement or interest. This could reflect limited customer interaction or follow-up, which may negatively affect overall business performance and the quality of customer insights. Addressing this issue is crucial for improving feedback rates and gaining a clearer understanding of customer experiences."],"metadata":{"id":"eiWhdyHcg3I0"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# Convert month to string/object instead of categorical if you don't want it to change\n","fk_df['month'] = fk_df['month'].astype('object')  # Ensure it's an object\n","\n","# If you want to create an ordered plot without converting to categorical\n","# Just create a sorted list of months\n","ordered_months = ['January', 'February', 'March', 'April', 'May', 'June',\n","                  'July', 'August', 'September', 'October', 'November', 'December']\n","\n","# Group by month and count orders\n","trends_over_month = fk_df['month'].value_counts().reindex(ordered_months, fill_value=0)\n","\n","sns.set(style='whitegrid')\n","\n","plt.figure(figsize=(12, 6))\n","plt.fill_between(trends_over_month.index, trends_over_month, color='skyblue', alpha=0.4)\n","plt.plot(trends_over_month.index, trends_over_month, marker='.', linestyle='-', color='b', linewidth=2)\n","\n","# Plotting chart\n","plt.xlabel('Month', fontsize=14)\n","plt.ylabel('Order Count', fontsize=14)\n","plt.title('Trends Over Month', fontsize=16)\n","plt.xticks(rotation=45, fontsize=12)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n","\n","# If it needs to be categorical again for some reason, convert it back after plotting\n","fk_df['month'] = fk_df['month'].astype('object')  # Convert back to object if needed"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["I had picked line chart to show the trends of order date time over months."],"metadata":{"id":"TDFhpfPZg-qX"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["The line chart illustrates a noticeable surge in order bookings between June and August, with a particularly sharp increase leading into August. During this period, August stands out as the peak month with the highest volume of order bookings, indicating a strong seasonal trend or promotional influence."],"metadata":{"id":"O15BLoLThB-U"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["The gained insights will certainly help in creating a positive business impact. The observed peak in order bookings from June to August highlights a key period of increased demand. By leveraging this information, businesses can focus their marketing efforts, promotional campaigns, and resource allocation during these months to maximize sales."],"metadata":{"id":"Y3um5wQQhFjs"}},{"cell_type":"markdown","source":["However, the insight that order bookings are only decent during the rest of the year, and do not match the high volumes seen between June and August, could be a concern for maintaining consistent growth. The relatively lower order bookings outside of this peak season may indicate periods of stagnation or slower business activity. If not addressed, these quieter periods could hinder overall annual growth."],"metadata":{"id":"8y0vcH4FhG46"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# Group by day and count orders\n","orders_day = fk_df.groupby('day')['Order_id'].count()\n","\n","# Set the figure size correctly\n","plt.figure(figsize=(12, 6))\n","\n","# Create the bar plot\n","sns.barplot(x=orders_day.index, y=orders_day.values, palette='GnBu')\n","\n","# Customize the chart\n","plt.title('Orders by Day of the Week', fontsize=16)\n","plt.xlabel('Day of the Week', fontsize=14)\n","plt.ylabel('Number of Orders', fontsize=14)\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.tight_layout()\n","plt.grid(False)\n","plt.show()"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["The bar chart was chosen to illustrate the number of orders placed on each day of the week."],"metadata":{"id":"5eYNIynBh402"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["The chart illustrates that the highest volume of orders is recorded on Sunday, making it the most popular day for bookings. This is followed closely by Tuesday and Friday, which also show significant order counts. In contrast, Monday stands out as the day with the fewest bookings, highlighting a potential area for improvement in customer engagement at the start of the week."],"metadata":{"id":"dVRNEL6Oh76t"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["The insights gained can significantly enhance business outcomes. By identifying peak order periods, such as Sundays, businesses can tailor marketing strategies and optimize staffing to meet demand effectively."],"metadata":{"id":"ZfIzBTcVh_GU"}},{"cell_type":"markdown","source":["However, low order counts on Mondays may indicate customer disengagement, suggesting a need to reevaluate promotional efforts for that day. Addressing these insights can lead to improved customer satisfaction and foster growth."],"metadata":{"id":"1lkY0cYih-3g"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","# Grouping by day and count of orders\n","orders_day = fk_df.groupby('hour')['Order_id'].count()\n","\n","plt.figure(figsize=(13, 6))\n","\n","sns.barplot(x=orders_day.index, y=orders_day.values, palette='viridis')\n","\n","plt.plot(orders_day.index, orders_day.values, color='orange', marker='.', linestyle='-', linewidth=1, label='Trend')\n","\n","# Ploting chart\n","plt.title('Orders by Hour', fontsize=16)\n","plt.xlabel('Hour of the Day', fontsize=14)\n","plt.ylabel('Number of Orders', fontsize=14)\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.legend()  # Add a legend to distinguish between bar and line\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["I selected a bar chart to analyze the trends in order placements over a 24-hour period."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["The chart illustrates that the evening hours experience the highest volume of order placements, particularly peaking at 18:00. This time is notably the busiest for orders, followed closely by 10:00, 19:00, and 20:00, which also see significant activity. This trend underscores a customer tendency to place orders later in the day, indicating that targeting marketing efforts or promotions during these peak evening hours could effectively engage customers and enhance sales."],"metadata":{"id":"EQyFWZ3HiatS"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["The insights from the analysis of order placements indicate a potential for positive business impact. With peak order times identified in the evening, especially at 18:00, businesses can target marketing efforts and promotions during these hours to effectively engage customers and boost sales."],"metadata":{"id":"u5VlZ3BNiuXM"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","# Calculating the distribution of product categories\n","product_category_dist = fk_df['Product_category'].value_counts()\n","\n","# Filtering out 'Not Available'\n","product_category_dist = product_category_dist[product_category_dist.index != 'Not Available']\n","\n","# Calculating the percentage distribution\n","percentage_distribution = (product_category_dist / product_category_dist.sum()) * 100\n","\n","percentage_distribution = percentage_distribution.sort_values(ascending=True) # sorting in ascending order\n","\n","plt.figure(figsize=(12, 6))\n","plt.barh(percentage_distribution.index, percentage_distribution, color='lightblue')\n","plt.grid(False)\n","plt.ylabel('Product Category')\n","plt.xlabel('Percentage')\n","plt.title('Distribution of Product Category (Percentage)')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["\"I have chosen Vertical bar chart to illustrate the percentage distribution of each product category. This visualization allows us to see the relative proportion of each category within the overall product lineup, highlighting which categories are more prevalent and how they compare to one another in terms of percentage.\""],"metadata":{"id":"Ui5wF5lXi_Ah"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["The chart shows that Electronics accounts for more than 25% of the total, while Lifestyle makes up 24%. In contrast, Furniture, Affiliates, and Gift Cards have the lowest percentage shares"],"metadata":{"id":"aN3p5YeujEdB"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["The insights reveal that Electronics and Lifestyle categories perform well, guiding marketing and inventory strategies. However, the low percentages for Furniture, Affiliates, and Gift Cards indicate underperformance, which may require increased marketing or adjustments to improve growth and customer interest."],"metadata":{"id":"wlkKLygpjJpP"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","top_agent = fk_df['Agent_name'].value_counts().nlargest(10)\n","plt.figure(figsize = (10, 6))\n","sns.barplot(x = top_agent.values, y = top_agent.index, palette = 'inferno')\n","plt.title('Top 10 Agent', fontsize = 14)\n","plt.xlabel('Number of Occurrences', fontsize = 12)\n","plt.ylabel('Agent Name', fontsize = 12)\n","plt.grid(False)\n","plt.show()"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["I had picked bar-chart to identify the top Agent."],"metadata":{"id":"2UYbTIsAjTDk"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["The chart reveals that Agent Wendy Taylor is the most occured Agent where as the Agent Timothy Huff is the second largest time occured Agent name."],"metadata":{"id":"oo7sU6wtjZaP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["The chart shows Agent Wendy Taylor as the most frequently occurring agent, with Agent Timothy Huff in second place. These insights suggest leveraging Wendy’s expertise could enhance team performance and client satisfaction. However, heavy reliance on Wendy might lead to burnout, and Timothy’s lower ranking could indicate performance issues. Addressing these concerns with balanced workloads and targeted training can prevent negative impacts and foster positive growth."],"metadata":{"id":"VOkpPws2jem2"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["# Chart - 11 visualization code\n","total_counts = fk_df.groupby('Supervisor')['Agent Shift'].count().nlargest(10)\n","\n","# Plotting the pie chart\n","plt.figure(figsize=(8, 5))  # Adjust figure size if needed\n","total_counts.plot(kind='pie', autopct='%1.1f%%', colormap='Spectral', startangle=90,\n","                  wedgeprops={'edgecolor': 'none'})  # Remove white border\n","\n","plt.title('Proportion of Total Shifts by Top 10 Supervisor', fontsize=16)\n","plt.ylabel('')  # Remove y-label\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["I had picked Pie-chart to identify the distribution of shifts by the top 10 Supervisor."],"metadata":{"id":"J3kMahPXjpSH"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["The chart indicates that Supervisor \"Carter Park\" has the highest occurrence time, accounting for 12.7%. This is followed by \"Elijah Yamaguchi\" at 10.7%, and both \"Nathan Patel\" and \"Zoe Yamamoto\" each at 10.4%."],"metadata":{"id":"kPPWCxdgjsuk"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["The insights indicate that understanding supervisor time allocations can enhance operational efficiency and customer satisfaction. For example, leveraging \"Carter Park's\" high occurrence time can optimize task management and resource allocation."],"metadata":{"id":"j8Wx8PxNjyBg"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["# Chart - 12 visualization code\n","# Calculating distribution of CSAT scores\n","csat_distribution = fk_df['CSAT Score'].value_counts()\n","\n","colors = ['#66c2a5', '#fc8d62', '#8da0bb', '#e78ac3', '#a6d854']  # Modify as needed for more colors\n","\n","# Plot the pie chart\n","plt.figure(figsize=(8, 6))\n","plt.pie(csat_distribution, labels=csat_distribution.index, autopct='%1.1f%%', colors=colors, startangle=140)\n","\n","# Add title\n","plt.title('Distribution of CSAT Score', fontsize=16)\n","\n","# Show the plot\n","plt.tight_layout()  # Adjust layout for better appearance\n","plt.show()"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["Pie-chart i have choosen in order to analyse the CSAT Score distribution."],"metadata":{"id":"cjlOPdHOj9h3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["The pie chart demonstrates that 68.9% of responses indicate a CSAT Score of 5, signifying a high level of customer satisfaction. Conversely, CSAT Scores of 1 account for 13.4% of the responses, representing a minority of customers who report lower or very low satisfaction, while CSAT Scores of 4 make up 13.1%. This distribution emphasizes that most customers are highly satisfied, whereas a smaller segment experiences lower satisfaction levels."],"metadata":{"id":"g5M6NbiCkBA4"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["Positve Impact: The insights gained from the CSAT Score distribution indicate that a significant majority of customers are highly satisfied, with 68.9% scoring a 5. This level of customer satisfaction can lead to positive business outcomes, such as increased customer loyalty, higher retention rates, and positive word-of-mouth referrals, ultimately driving sales and enhancing brand reputation."],"metadata":{"id":"EeDMscTOkFKO"}},{"cell_type":"markdown","source":["Negative Impact: The presence of 13.4% of responses scoring a 1 indicates a segment of customers experiencing very low satisfaction. This negative feedback could signal underlying issues that, if unaddressed, may lead to decreased customer loyalty and potential churn."],"metadata":{"id":"oEjglkD6kJQU"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["# Chart - 13 visualization code\n","# Filtering  out the \"Not Available\" product category\n","filtered_fk_df = fk_df[fk_df['Product_category'] != 'Not Available']\n","\n","plt.figure(figsize=(15, 7))\n","sns.barplot(x='Product_category', y='CSAT Score', data=filtered_fk_df, palette='pastel')  # Change to a different color palette\n","plt.xlabel('Product Category', fontsize=14)\n","plt.ylabel('Average CSAT Score', fontsize=14)\n","plt.title('Average CSAT Score by Product Category', fontsize=16)\n","plt.xticks(rotation=45, fontsize=12)\n","plt.tight_layout()\n","plt.grid(False)\n","plt.show()"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["I have choose bar-chart to identify the CSAT Score across the different Product categories."],"metadata":{"id":"QkhctMjAkRs7"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["The chart shows that the Electronics, Lifestyle, Affiliates and Home categories have average CSAT Scores above 4, indicating high customer satisfaction. In contrast, the Mobiles and Furniture categories have average CSAT Scores of 3.5, suggesting lower customer satisfaction in these areas."],"metadata":{"id":"L2Iy1dqmkXHc"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["The insights show high CSAT Scores above 4 for Electronics, Lifestyle, Affiliates and Home categories can boost customer loyalty and sales. However, the lower CSAT Scores of 3.5 for Mobiles and Furniture indicate areas needing improvement. Addressing these lower scores is essential to prevent negative feedback and potential loss of customers."],"metadata":{"id":"QrA9pHrfka7g"}},{"cell_type":"markdown","source":["#### Chart - 14"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Chart - 14 visualization code\n","# Get the distribution of Agent Shift\n","agent_shift_distribution = fk_df['Agent Shift'].value_counts()\n","\n","# Plot the donut chart\n","plt.figure(figsize=(6, 8))\n","plt.pie(agent_shift_distribution,\n","        labels=agent_shift_distribution.index,\n","        autopct='%1.1f%%',\n","        colors=plt.cm.Set1.colors,\n","        startangle=140,\n","        wedgeprops={'width': 0.3})\n","\n","# Title\n","plt.title('Distribution of Agent Shift')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["I have picked Donut chart to show the distribution of Agent Shift."],"metadata":{"id":"LJKoqr0pkjDw"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["The chart reveals that the morning shift comprises 48.6% of the total, while the evening shift represents 39.1%. The afternoon shift makes up a smaller portion, accounting for just 6.8%."],"metadata":{"id":"UXINtF5kkmNF"}},{"cell_type":"markdown","source":["**3. Will the gained insights help creating a positive business impact****?**"],"metadata":{"id":"zgITTvD2ksAE"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"mClX1sEyk6eP"}},{"cell_type":"markdown","source":["The insights show that the morning shift (48.6%) and evening shift (39.1%) are well-utilized, which can enhance operational efficiency and customer satisfaction. However, the afternoon shift (6.8%) is underutilized, potentially leading to gaps in coverage or productivity. Addressing this imbalance is crucial to avoid negative impacts and ensure comprehensive service."],"metadata":{"id":"iPTZF9ojlFA5"}},{"cell_type":"markdown","source":["#### Chart - 15"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# chart - 13 visualization code\n","\n","# Filtering out \"Not Available\" entries from both Product_category and Sub-category\n","filtered_fk_df = fk_df[(fk_df['Product_category'] != 'Not Available') &\n","                        (fk_df['Sub-category'] != 'Not Available')]\n","\n","# Ploting chart\n","plt.figure(figsize=(16, 12))\n","sns.scatterplot(data=filtered_fk_df, x='Product_category', y='Sub-category', hue='CSAT Score', palette='viridis', s=100)\n","\n","plt.title('Product Category vs Sub-category',  fontsize=14)\n","plt.xlabel('Product Category')\n","plt.ylabel('Sub-category')\n","plt.grid(False)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["I chose a scatter plot to visualize the relationship between product categories and subcategories."],"metadata":{"id":"pefblvg9l2sS"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["I chose a scatter plot to visualize the relationship between product categories and subcategories."],"metadata":{"id":"mFRuOyI9l5p7"}},{"cell_type":"markdown","source":["#### Chart - 16"],"metadata":{"id":"QJ8wn4XNmICU"}},{"cell_type":"code","source":["# chart - 14 visualization code\n","\n","# Filtering out \"Not Available\" entries from 'Product_category'\n","filtered_fk_df = fk_df[fk_df['Product_category'] != 'Not Available']\n","\n","# Group the data by 'Product_category' and calculate the sum of 'Item_price'\n","revenue_generation = filtered_fk_df.groupby('Product_category')['Item_price'].sum()\n","\n","# Plot the bar plot\n","plt.figure(figsize=(14, 6))\n","sns.barplot(x=revenue_generation.index, y=revenue_generation.values, palette='mako')  # Added a color palette\n","\n","# Set the correct labels and title\n","plt.xlabel('Product Category', fontsize=12)\n","plt.ylabel('Total Revenue', fontsize=12)\n","plt.title('Revenue Generation by Product Category', fontsize=12)\n","\n","# Rotate the x-axis labels for better readability\n","plt.xticks(rotation=45)\n","\n","# Format y-axis to display as integers (without decimal places)\n","plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n","plt.grid(False)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_uPFD3cHmTw6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Why did you pick the specific chart?"],"metadata":{"id":"EQ6D6nSNmeFe"}},{"cell_type":"markdown","source":["I have choose a Bar-chart to identify the revenue generated by each product category."],"metadata":{"id":"95UDfyUhmioH"}},{"cell_type":"markdown","source":["2. what is/are the insight(s) found from the chart?"],"metadata":{"id":"hcksRGoBmpeO"}},{"cell_type":"markdown","source":["The chart reveals that the Mobile category generates the highest revenue, exceeding 4 crore, followed by Electronics with 2.75 crore, Home Appliances with 1.7 crore, and Lifestyle with 45 lakhs. Conversely, categories such as Books & General merchandise, Home, and Furniture generate the least revenue."],"metadata":{"id":"WCnze6CLmuxz"}},{"cell_type":"markdown","source":["3. Will the gained insights help creating a positive business impact?"],"metadata":{"id":"iumIra_Dm0AI"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3lvDxs5Mm27n"}},{"cell_type":"markdown","source":["The insights show that the Mobile category drives the highest revenue, suggesting a focus on this area can boost overall growth. Electronics and Home Appliances also perform well, which can guide resource allocation and marketing strategies. However, low revenue from categories like Lifestyle, Home, and Furniture indicates potential issues. Addressing these underperforming categories is essential to prevent negative growth and enhance overall profitability."],"metadata":{"id":"ihoMg84bm37g"}},{"cell_type":"markdown","source":["#### Chart -17 - Correlation Matrix Heatmap"],"metadata":{"id":"voLbSopsm7VL"}},{"cell_type":"code","source":["# Chart - 16 visualization code\n","numeric_data = fk_df.select_dtypes(include=['number'])\n","\n","# Computing correlation matrix for the numerical data\n","correlation_matrix = numeric_data.corr()\n","plt.figure(figsize = (12, 8))\n","sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='Spectral', linewidths=0.5)\n","plt.title('Correlation Matrix Heatmap')\n","plt.show()"],"metadata":{"id":"aoDBYTy3nDHZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.  Why did you pick the specific chart?"],"metadata":{"id":"1lxg8TSRnCGm"}},{"cell_type":"markdown","source":["The purpose to pick correlation heatmap is to visualize the relation between each numeric variables."],"metadata":{"id":"0GAOFtPqnK5q"}},{"cell_type":"markdown","source":["2.  What is/are the insight(s) found from the chart?"],"metadata":{"id":"6p5XaGVbnRu-"}},{"cell_type":"markdown","source":["The chart does not show any strong relationships between the variables."],"metadata":{"id":"fnt1299YnXZb"}},{"cell_type":"markdown","source":["#### Chart - 18 -Pair plot"],"metadata":{"id":"ccvltasOndI9"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","sns.pairplot(fk_df)\n","plt.grid(False)\n","plt.show()"],"metadata":{"id":"TYV23O23nxrM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.  Why did you pick the specific chart?"],"metadata":{"id":"6gLKHE0QnlA1"}},{"cell_type":"markdown","source":["I picked the pairplot because it provides a clear view of relationships between multiple variables at once. This visualization helps identify correlations and patterns across different features, reveals how variables interact with each other, and highlights distributions and potential outliers."],"metadata":{"id":"Pk8Wvxaxn2AL"}},{"cell_type":"markdown","source":["2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"bScjpOeAn3NU"}},{"cell_type":"markdown","source":["The pair-plot chart does not reveals any strong relationship between the variables."],"metadata":{"id":"awW5N9Uqn-pK"}},{"cell_type":"markdown","source":["## **5. Hypothesis Testing**"],"metadata":{"id":"T4PYmZrTo3WD"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"LFVSKXUopMhT"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"oLKehG53pOyv"}},{"cell_type":"markdown","source":["##### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"5KdwZRA5pWl1"}},{"cell_type":"markdown","source":["From the distribution of CSAT scores, I found that the mean CSAT score is 4.22. Let's use hypothesis testing to see if this statement is true."],"metadata":{"id":"TdpL75-IpepK"}},{"cell_type":"markdown","source":["Null Hypothesis (H₀): μ = 4.22"],"metadata":{"id":"xXUVIgp9phWk"}},{"cell_type":"markdown","source":["Alternate Hypothesis (H₁): μ ≠ 4.22"],"metadata":{"id":"YeWMD8mKpnnm"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"o2_fpQsIppYY"}},{"cell_type":"code","source":["from scipy.stats import ttest_1samp\n","\n","# Test data for CSAT Score\n","test_data2 = fk_df['CSAT Score']\n","\n","# Hypothesized mean is 4.24\n","hypothesized_mean = 4.22\n","\n","# Conduct one-sample t-test\n","stats, p = ttest_1samp(test_data2, hypothesized_mean)\n","\n","print('t-statistic = %.2f, p-value = %.3f' % (stats, p))\n","\n","alpha = 0.05\n","if p < alpha:\n","    print(\"Reject null hypothesis: The mean CSAT score is significantly different from 4.22.\")\n","else:\n","    print(\"Fail to reject the null hypothesis: The mean CSAT score is not significantly different from 4.22.\")\n"],"metadata":{"id":"KbsghZ5Wp4-k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"X7m2PmXyp3zR"}},{"cell_type":"markdown","source":["I have conducted One sample t-test to obtain p-value"],"metadata":{"id":"XvfKL6OrqAgu"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"hVqQngbJqI6j"}},{"cell_type":"markdown","source":["I chosen one-sample t-test because I wanted to determine if the mean CSAT score in my dataset is significantly different from 4.22. This test is ideal for comparing the sample mean to a specific value, and it’s suitable for my data to check if it deviates from the hypothesized population mean."],"metadata":{"id":"mVemh6rmqL9g"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"q4y_KV1gqUPv"}},{"cell_type":"markdown","source":["##### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"m6JYly8yqYUm"}},{"cell_type":"markdown","source":["Item_price mean is 1439.22 in the data set, Let us conduct a Hypothesis testing to know the mean value of Item_price is significantly different or not."],"metadata":{"id":"4eWoRBKoqaqu"}},{"cell_type":"markdown","source":["Null Hypothesis (H₀): μ = 1439.22"],"metadata":{"id":"BrFTaADmqdHM"}},{"cell_type":"markdown","source":["Alternate Hypothesis (H₁): μ ≠ 1439.22"],"metadata":{"id":"GYu6CgAnqgKD"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"d4vvKEdgqjLb"}},{"cell_type":"code","source":["from scipy.stats import ttest_1samp\n","\n","# Define the hypothesized mean\n","hypothesized_mean = 1439.22\n","\n","# Perform one-sample t-test\n","test_data1 = fk_df['Item_price']\n","t_stat, p_value = ttest_1samp(test_data1, hypothesized_mean)\n","\n","# Print the t-statistic and p-value\n","print('t-statistic = %.2f, p-value = %.3f' % (t_stat, p_value))\n","\n","# Define significance level (commonly 0.05)\n","alpha = 0.05\n","\n","# Make a decision based on p-value\n","if p_value <= alpha:\n","    print(\"Reject null hypothesis: The mean of Item_price is significantly different from 1439.22.\")\n","else:\n","    print(\"Fail to reject null hypothesis: The mean of Item_price is not significantly different from 1439.22.\")\n","\n"],"metadata":{"id":"FQjVM8thqsCW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"8tykAvwuqvvN"}},{"cell_type":"markdown","source":["I have conducted one sample t-test to obtain the p-value."],"metadata":{"id":"4vaQiqwgq0-j"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"mcGNtGCKq7fa"}},{"cell_type":"markdown","source":["I chose the one-sample t-test because I wanted to know if the mean Itme_price in my dataset is significantly different from 1439.22. This test is ideal for comparing the sample mean to a specific value, and it’s suitable for my data to check if it deviates from the hypothesized population mean."],"metadata":{"id":"ZMO0XEG7rAIS"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"xfXN7YMcrGq8"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"ZQ8Q21hprN3D"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","print(fk_df.isnull().sum())"],"metadata":{"id":"Q3JY-z-rrQel"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"oW1_rg9orR6v"}},{"cell_type":"markdown","source":["The dataset currently contains no null values. However, if null values were to be introduced, we could use suitable imputation methods to fill them in. The choice of imputation technique would depend on the nature and distribution of the data, ensuring that we maintain the dataset's integrity and accuracy."],"metadata":{"id":"ugJb_41prWjM"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"ktYDYz55rYv0"}},{"cell_type":"markdown","source":["From EDA we know that the Item Price has 5.02 % Outliers."],"metadata":{"id":"1FSRoz0QrhrM"}},{"cell_type":"code","source":["# Calculate Q1 and Q3 while excluding zeros\n","Q1 = fk_df[fk_df['Item_price'] != 0]['Item_price'].quantile(0.25)\n","Q3 = fk_df[fk_df['Item_price'] != 0]['Item_price'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Determine the lower and upper bounds for outliers, excluding zeros\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Identify outliers without creating a new DataFrame\n","outliers = fk_df[(fk_df['Item_price'] < lower_bound) | (fk_df['Item_price'] > upper_bound) & (fk_df['Item_price'] != 0)]\n","\n","# Output the count of outliers\n","print(f\"Number of outliers: {len(outliers)}\")"],"metadata":{"id":"EjsiyqwTrs2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate percentage of outliers\n","outlier_percentage = (len(outliers) / len(fk_df)) * 100\n","print(f\"Percentage of outliers in Item_price: {outlier_percentage:.2f}%\")"],"metadata":{"id":"gEIeJicKrwB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Plot CSAT Score distribution before clipping\n","plt.figure(figsize=(10, 6))\n","sns.histplot(data=fk_df, x='Item_price', bins=25, kde=True)\n","plt.title('Item_price Distribution with Outliers')\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.show()"],"metadata":{"id":"4mB2laMer1XF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fk_df['Item_price'] = fk_df['Item_price'].clip(lower=lower_bound, upper=upper_bound)"],"metadata":{"id":"p91_Z4y5r5zG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_outliers = fk_df[(fk_df['Item_price'] < lower_bound) | (fk_df['Item_price'] > upper_bound)]\n","new_outlier_percentage = (len(new_outliers) / len(fk_df)) * 100\n","print(f\"Percentage of outliers in Item_price after clipping: {new_outlier_percentage:.2f}%\")\n"],"metadata":{"id":"5o32Be7Hr8AG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","sns.histplot(data=fk_df, x='Item_price', bins=25, kde=True)\n","plt.title('Item_price Distribution After Clipping Outliers')\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.show()"],"metadata":{"id":"Vk0p2e86r-gJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"giFyHiWNsS2L"}},{"cell_type":"markdown","source":["Since 5.02% of the Item_price values are identified as outliers, I opted for the IQR method to detect these extreme values. The IQR technique is a robust approach that focuses on the spread of the middle 50% of the data, making it less influenced by extreme values compared to methods like standard deviation.\n","\n","To calculate the IQR, I determined the difference between the 75th percentile (Q3) and the 25th percentile (Q1). Any values falling outside the calculated bounds were flagged as outliers, which helped me identify potential anomalies in the Item_price data.\n","\n","I established the upper and lower bounds, and to address these outliers, I employed clipping. This technique allowed me to adjust the outlier values so they remain within a specified range without completely removing them from the dataset."],"metadata":{"id":"FzzF43Ousatd"}},{"cell_type":"markdown","source":["### 9. HandlinG Imbalanced Dataset"],"metadata":{"id":"D1xuGu54sppK"}},{"cell_type":"code","source":["# Filter to keep only valid CSAT Scores between 1 and 5\n","fk_df = fk_df[fk_df['CSAT Score'].between(1, 5)]\n","\n","# Ensuring that our target variable remains int.\n","fk_df['CSAT Score'] = fk_df['CSAT Score'].astype(int)"],"metadata":{"id":"Pd32xyx4_u72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fk_df.drop(columns=['Unique id'], inplace=True)"],"metadata":{"id":"wUrA7fOj_y9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = fk_df.drop(columns=['CSAT Score'])\n","y = fk_df['CSAT Score']"],"metadata":{"id":"bvT5NFFF_2VZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.value_counts().reset_index()"],"metadata":{"id":"Iq2gDfpm_4Uz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"3B532PBT_6fS"}},{"cell_type":"markdown","source":["The dataset is considered imbalanced because there is a substantial difference in the number of samples across the various classes. Specifically, the majority class (Class 5) has significantly more samples than the minority class (Class 4), which also has a considerably higher number of samples than Class 3. This pattern continues, with Class 3 having more samples than Class 2, and so on."],"metadata":{"id":"1ELeDvcG__Dt"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","from imblearn.over_sampling import RandomOverSampler\n","\n","# Initialize RandomOverSampler\n","oversample = RandomOverSampler(random_state=42)\n","\n","# Apply oversampling to training data\n","X, y  = oversample.fit_resample(X, y)"],"metadata":{"id":"D7-GRQegAByt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape, y.shape"],"metadata":{"id":"knEkiQrcAMjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"gx640UMdAIh5"}},{"cell_type":"markdown","source":["To address the imbalance in the dataset, I utilized RandomOverSampler. This approach balances the dataset by randomly replicating samples from the minority classes until all classes are represented equally."],"metadata":{"id":"MiL3ghluARSu"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"nayxAYXtAWEX"}},{"cell_type":"markdown","source":["The optimal strategy for data preprocessing begins with splitting the dataset into training and test sets. Once the split is complete, encoding and scaling should be applied separately to the training and test sets. This approach ensures that the preprocessing steps do not unintentionally include information from the test set during the training process."],"metadata":{"id":"wX5NOD5FAaoV"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"tOzkiDdtAe4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import label_binarize\n","# Binarize y_test for ROC curve calculation (for multi-class problems)\n","y_test_binarized = label_binarize(y_test, classes=[1, 2, 3, 4, 5])  # Ensure all classes 1-5 are included"],"metadata":{"id":"gBq3nWTGAks-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training set shape:\", X_train.shape, y_train.shape)\n","print(\"Testing set shape:\", X_test.shape, y_test.shape)"],"metadata":{"id":"2MiAplKIAn-H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What data splitting ratio have you used and why?"],"metadata":{"id":"MsM7IexhArhP"}},{"cell_type":"markdown","source":["I opted for an 80-20 train-test split because it provides a good balance between having enough data for effective model training (80%) and ensuring a sufficient portion for reliable model evaluation (20%). This ratio is widely used and facilitates the assessment of the model's performance on unseen data."],"metadata":{"id":"Wo1uUmn0AvX4"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"LKqjG-JmAxMc"}},{"cell_type":"code","source":["!pip install category_encoders"],"metadata":{"id":"7Z8FqXynA3ZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Identify categorical features with high cardinality (more than 30 unique values)\n","cat_features_high_car = [column for column in fk_df.columns if fk_df[column].dtype == 'object' and len(fk_df[column].unique()) > 30]\n","\n","# Identify categorical features with low cardinality (30 or fewer unique values)\n","cat_features = [column for column in fk_df.columns if fk_df[column].dtype == 'object' and len(fk_df[column].unique()) <= 30]\n","\n","# Import BinaryEncoder for high-cardinality features and necessary transformers from sklearn\n","from category_encoders import BinaryEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","# Initialize BinaryEncoder for high-cardinality categorical features\n","encoder = BinaryEncoder(cols=cat_features_high_car)\n","\n","# Apply Binary Encoding to the training and test data\n","X_train = encoder.fit_transform(X_train)\n","X_test = encoder.transform(X_test)\n","\n","# Initialize OneHotEncoder for low-cardinality categorical features\n","onehotencoder = OneHotEncoder(drop='first', sparse_output=False, dtype=np.int64)\n","\n","# Create a ColumnTransformer to apply OneHotEncoder to low-cardinality features\n","preprocessor = ColumnTransformer(\n","    [\n","        (\"OneHotEncoder\", onehotencoder, cat_features),  # Apply OneHotEncoder to low-cardinality features\n","    ], remainder='passthrough'  # Pass through other columns without transformation\n",")\n","\n","# Apply One-Hot Encoding to low-cardinality features and pass through high-cardinality features\n","X_train = preprocessor.fit_transform(X_train)\n","X_test = preprocessor.transform(X_test)\n","\n","# Convert the transformed arrays back to DataFrames with feature names\n","X_train = pd.DataFrame(X_train, columns=preprocessor.get_feature_names_out())\n","X_test = pd.DataFrame(X_test, columns=preprocessor.get_feature_names_out())"],"metadata":{"id":"hiCR3C42A8bS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.head()"],"metadata":{"id":"DamHbO0ABHWv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"XaWHHfB_BLWs"}},{"cell_type":"markdown","source":["I used both One-Hot Encoding and Binary Encoding for categorical columns in my dataset.\n","\n","1. One-Hot Encoding: Applied to categorical columns without an inherent order or ranking, such as channel_name, category, Sub-category,Customer Remarks, Customer_City etc.\n","\n","2. Binary Encoding: Used for categorical columns with high cardinality, such as Item_price, Agent_name, Supervisor, Manager, Agent Shift. Binary Encoding was chosen to manage the large number of features that One-Hot Encoding would have introduced"],"metadata":{"id":"ntTDpB45BPZ_"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"Y4i_jrfYBX0P"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"sOFZMCP-Beja"}},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"_90RKi59Bj1H"}},{"cell_type":"markdown","source":["What all feature selection methods have you used and why?"],"metadata":{"id":"B6JPClwLBpsz"}},{"cell_type":"markdown","source":["I have droped Unique id column because it lacks significant predictive power."],"metadata":{"id":"6SHFajySBtVX"}},{"cell_type":"markdown","source":["Which all features you found important and why?"],"metadata":{"id":"GGJFYk1BBvlB"}},{"cell_type":"markdown","source":["1. Channel Name: Different channels may influence customer engagement levels and their likelihood of making a purchase.\n","\n","2. Product Category: Various product categories can attract different customer segments with distinct preferences and interests.\n","\n","3. Customer Remarks: Feedback from customers can provide insights into their satisfaction and concerns, influencing their future purchasing decisions.\n","\n","4. Order ID: While not predictive by itself, it helps in tracking customer transactions and behaviors.\n","\n","5. Order Date Time: The timing of orders may reveal trends related to peak purchasing times or seasonal interests.\n","\n","6. Issue Reported At: The timestamp of reported issues can indicate common pain points and the urgency of customer needs.\n","\n","7. Issue Responded: The response time for issues may affect customer satisfaction and loyalty.\n","\n","8. Survey Response Date: This feature is crucial for understanding the timing of customer feedback and its relation to their experience.\n","\n","9. Customer City: The location of customers can influence their purchasing habits and preferences based on regional factors.\n","\n","10. Item Price: The price of items plays a significant role in customer interest and purchasing decisions.\n","\n","11. Agent Name: Different agents may have varying success rates in closing sales, impacting overall performance.\n","\n","12. Supervisor and Manager: These roles might reflect the support structure behind sales, which could influence customer interactions.\n","\n","13. Tenure Bucket: This indicates how long customers have been with the company, which can correlate with their loyalty and purchasing behavior.\n","\n","14. Agent Shift: The timing of agent shifts might affect customer availability and engagement."],"metadata":{"id":"sco7S9MSB3Ty"}},{"cell_type":"markdown","source":["#### 6. Data Scaling"],"metadata":{"id":"h2OYv0tjClha"}},{"cell_type":"code","source":["# Scaling your data\n","num_columns = [column for column in X_train.columns if X_train[column].dtype != \"object\" and len(X_train[column].unique()) > 10]\n","\n","# Import StandardScaler from sklearn\n","from sklearn.preprocessing import StandardScaler\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit the scaler on the training data and transform the numerical columns\n","X_train[num_columns] = scaler.fit_transform(X_train[num_columns])\n","\n","# Transform the numerical columns of the test data using the fitted scaler\n","X_test[num_columns] = scaler.transform(X_test[num_columns])"],"metadata":{"id":"58qw7RD7Cp3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the original feature column names\n","features_columns = X_train.columns\n","\n","# Convert the DataFrame to a NumPy array\n","# X_train = X_train.values\n","\n","# Convert the test set DataFrame to a NumPy array\n","# X_test = X_test.values\n"],"metadata":{"id":"-9PBm7DCCuwa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Which method have you used to scale you data and why?"],"metadata":{"id":"Znk0JqokCyZ0"}},{"cell_type":"markdown","source":["I used Standard Scaling (via StandardScaler) because it normalizes features to have a mean of 0 and a standard deviation of 1. This ensures equal contribution from all features, improves convergence for algorithms, and handles features with different units effectively."],"metadata":{"id":"XUMIPohPC6Ab"}},{"cell_type":"markdown","source":["### 7. ML Model Implementation"],"metadata":{"id":"pS0NyI5zC9lH"}},{"cell_type":"code","source":["# Set default behavior to not show grid lines for all plots\n","plt.rcParams['axes.grid'] = False  # Disable grid lines globally\n","\n","# Then proceed to create your plots"],"metadata":{"id":"pWTrDApZDLCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# << ML Model Building >>\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, RocCurveDisplay\n"],"metadata":{"id":"P1PjlhnhDWWj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model : 1 (Decision Tree)"],"metadata":{"id":"MZ_iWVrPDYUT"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","model_1 = DecisionTreeClassifier()\n","\n","# Fit the Algorithm\n","model_1.fit(X_train, y_train)\n","# Predict on the model\n","model_1_pred = model_1.predict(X_test)"],"metadata":{"id":"uZrRtPtmDcw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test, model_1_pred))"],"metadata":{"id":"wVgnxWoYDipV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"Es3qMXrEDyjI"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","# Create a dictionary for evaluation metrics for model_2\n","evaluation_dict_model_1 = {\n","    'Model Name': 'model_1',\n","    'Model Type': 'Decision Tree Classifier',\n","    'Accuracy': accuracy_score(y_test, model_1_pred),\n","    'Recall': recall_score(y_test, model_1_pred, average='weighted'),\n","    'Precision': precision_score(y_test, model_1_pred, average='weighted'),\n","    'F1-Score': f1_score(y_test, model_1_pred, average='weighted')\n","}\n","\n","# Convert the dictionary into a DataFrame for easy visualization\n","evaluation_df_model_1 = pd.DataFrame(evaluation_dict_model_1, index=[0])\n","\n","# Output the DataFrame\n","evaluation_df_model_1"],"metadata":{"id":"7zjt_LbID3AZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the confusion matrix for the Decision Tree model\n","ConfusionMatrixDisplay.from_estimator(model_1, X_test, y_test)\n","\n","# Add a title\n","plt.title('Confusion Matrix for Decision Tree Classifier', fontweight='bold')\n","plt.grid(False)\n","# Display the plot\n","plt.show()"],"metadata":{"id":"xUGJ352jD6g7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Binarize the output labels for multi-class ROC curve calculation\n","n_classes = len(np.unique(y_test))  # Adjust according to your dataset\n","y_test_binarized = label_binarize(y_test, classes=np.arange(1, n_classes + 1))\n","\n","# Predict probabilities for the ROC curve using model_2\n","y_score = model_1.predict_proba(X_test)\n","\n","# Initialize a ROC curve plot\n","plt.figure()\n","\n","# For each class, compute the ROC curve\n","for i in range(n_classes):\n","    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'Class {i + 1} ROC (AUC = {roc_auc:.2f})')\n","\n","# Plot diagonal for random guessing\n","plt.plot([0, 1], [0, 1], 'k--')\n","\n","# Add labels and title\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve for Multi-Class Classification (Model 1) Before Cross-Validation ')\n","plt.legend(loc=\"lower right\")\n","\n","# Display the ROC curve plot\n","plt.show()"],"metadata":{"id":"BpU8nk5YEATJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Cross - Validation\n","\n","\n","\n"],"metadata":{"id":"QOurYsGJEC14"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Define the decision tree model\n","model_1 = DecisionTreeClassifier()\n","\n","# Perform cross-validation (cv=3 for 3-fold cross-validation)\n","cv_scores = cross_val_score(model_1, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n","\n","# Output the cross-validation scores\n","print(\"Cross-Validation Scores:\", cv_scores)\n","print(\"Mean CV Accuracy:\", cv_scores.mean())"],"metadata":{"id":"SIGygrIrEOQU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Which cross-validation optimization technique have you used and why?"],"metadata":{"id":"9w_I2diJDTDb"}},{"cell_type":"markdown","source":["I used K-Fold Cross-Validation to optimize the Decision Tree model. This technique was chosen because it ensures balanced evaluation by using each fold as a test set once while training on the remaining data. It provides a reliable estimate of model performance and stability, as shown by the mean accuracy of 0.84 and the variations in scores across folds. This method efficiently utilizes data and offers insights into the model’s generalizability. Additionally, the variation in accuracy scores across the folds ([0.787, 0.797, 0.937]) offers insights into the model’s consistency and stability."],"metadata":{"id":"SDlajsSkEXa9"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_predict\n","\n","# Perform cross-validation and predict probabilities\n","y_score_cv = cross_val_predict(model_1, X_test, y_test, cv=3, method='predict_proba')\n","\n","# Binarize the output labels for multi-class ROC curve calculation\n","n_classes = len(np.unique(y_test))  # Adjust according to your dataset\n","y_test_binarized = label_binarize(y_test, classes=np.arange(1, n_classes + 1))\n","\n","# Initialize a ROC curve plot\n","plt.figure()\n","\n","# For each class, compute the ROC curve after cross-validation\n","for i in range(n_classes):\n","    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score_cv[:, i])\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'Class {i + 1} ROC (AUC = {roc_auc:.2f})')\n","\n","# Plot diagonal for random guessing\n","plt.plot([0, 1], [0, 1], 'k--')\n","\n","# Add labels and title\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve for Multi-Class Classification (Model 1 after Cross-Validation)')\n","plt.legend(loc=\"lower right\")\n","\n","# Display the ROC curve plot\n","plt.show()"],"metadata":{"id":"R_WoNihoEd5p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Enhancements Following Cross-Validation:"],"metadata":{"id":"kx0WfBwcEf4N"}},{"cell_type":"markdown","source":["After performing 3-Fold Cross-Validation, the AUC values for the classes have shown notable changes. Specifically, the AUC for Class 1 decreased from 0.97 to 0.83, while Class 3's AUC decreased slightly from 0.98 to 0.96. Similarly, Class 4's AUC dropped from 0.97 to 0.83. Conversely, the AUC for Class 2 increased from 0.98 to 0.99, and Class 5 experienced a significant rise from 0.61 to 0.99. Overall the model has performed well for class 5."],"metadata":{"id":"D27WcqKyElUz"}},{"cell_type":"markdown","source":["### Model - 2 (Random Forest)"],"metadata":{"id":"sypa46GVEnPx"}},{"cell_type":"code","source":["# ML Model - 2 Implementation\n","model_2 = RandomForestClassifier()\n","# Fit the Algorithm\n","model_2.fit(X_train, y_train)\n","# Predict on the model\n","model_2_pred = model_2.predict(X_test)"],"metadata":{"id":"o8u-ZS2hE2NU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test, model_2_pred))"],"metadata":{"id":"Dm6lr163FMXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7_f2FZKQ31nG"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","# Create a dictionary for evaluation metrics\n","evaluation_dict = {\n","    'Model Name': 'model_2',\n","    'Model Type': 'Random Forest Classifier',\n","    'Accuracy': accuracy_score(y_test, model_2_pred),\n","    'Recall': recall_score(y_test, model_2_pred, average='weighted'),  # Use 'weighted' for multiclass\n","    'Precision': precision_score(y_test, model_2_pred, average='weighted'),  # Use 'weighted' for multiclass\n","    'F1-Score': f1_score(y_test, model_2_pred, average='weighted')  # Use 'weighted' for multiclass\n","}\n","\n","# Convert the dictionary into a DataFrame\n","evaluation_df = pd.DataFrame(evaluation_dict, index=[0])\n","\n","# Output the DataFrame\n","evaluation_df"],"metadata":{"id":"FdoLYaP634rV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display confusion matrix\n","ConfusionMatrixDisplay.from_estimator(model_2, X_test, y_test)\n","plt.title('Confusion Matrix (Random Forest)', fontweight='bold')\n","plt.show()"],"metadata":{"id":"6EqpGmKz39S_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Binarize the output labels for multi-class ROC curve calculation\n","n_classes = len(np.unique(y_test))\n","y_test_binarized = label_binarize(y_test, classes=np.arange(1, n_classes + 1))\n","\n","# Predict probabilities for the ROC curve\n","y_score = model_2.predict_proba(X_test)\n","\n","# Initialize a ROC curve plot\n","plt.figure()\n","\n","# For each class, compute the ROC curve\n","for i in range(n_classes):\n","    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'Class {i + 1} ROC (AUC = {roc_auc:.2f})')\n","\n","# Plot diagonal for random guessing\n","plt.plot([0, 1], [0, 1], 'k--')\n","\n","# Add labels and title\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve for Model 2 (Random Forest) Before Cross-Validation')\n","plt.legend(loc=\"lower right\")\n","\n","# Display the ROC curve plot\n","plt.show()"],"metadata":{"id":"lQ9nR4TH4ARA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Cross - Validation"],"metadata":{"id":"HvX1_iyF4Cdg"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","\n","# Perform cross-validation (cv=3 for 3-fold cross-validation)\n","cv_scores = cross_val_score(model_2, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n","\n","# Output the cross-validation scores\n","print(\"Cross-Validation Scores:\", cv_scores)\n","print(\"Mean CV Accuracy:\", cv_scores.mean())"],"metadata":{"id":"WLB3Dbp14Jwa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which cross-validation optimization technique have you used and why?"],"metadata":{"id":"oG47h5ex48CE"}},{"cell_type":"markdown","source":["I utilized K-Fold Cross-Validation for optimizing the Random Forest model. This technique was selected because it effectively balances model evaluation and computational efficiency.\n","\n","The scores obtained from cross-validation ([0.7938, 0.8329, 0.9756]) reflect the model's performance across different subsets, indicating a robust evaluation. The mean CV accuracy of approximately 0.87 demonstrates the model’s overall effectiveness and its ability to generalize well to unseen data. K-Fold Cross-Validation provides a comprehensive assessment of model stability and performance, making it a reliable choice for this analysis."],"metadata":{"id":"fkWcdU355EI9"}},{"cell_type":"code","source":["# Fit the model again on the cross-validated dataset (if necessary)\n","model_2.fit(X_train, y_train)\n","\n","# Predict probabilities for the ROC curve\n","y_score_cv = model_2.predict_proba(X_test)\n","\n","# Initialize a ROC curve plot\n","plt.figure()\n","\n","# For each class, compute the ROC curve\n","for i in range(n_classes):\n","    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score_cv[:, i])\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'Class {i + 1} ROC (AUC = {roc_auc:.2f})')\n","\n","# Plot diagonal for random guessing\n","plt.plot([0, 1], [0, 1], 'k--')\n","\n","# Add labels and title\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve for Model 3 (Random Forest) After Cross-Validation')\n","plt.legend(loc=\"lower right\")\n","\n","# Display the ROC curve plot\n","plt.show()"],"metadata":{"id":"6n_8-ObH5IyJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Enhancements Following Cross-Validation:"],"metadata":{"id":"kjK5jELK57of"}},{"cell_type":"markdown","source":["After conducting cross-validation, the AUC remained stable, indicating that the model's ability to distinguish between classes did not change. This consistency suggests that the model is reliable and robust, as it maintained its performance level across different subsets of the data. The unchanged AUC also implies that the cross-validation process effectively validated the model without introducing any significant variations in its predictive capability."],"metadata":{"id":"uhSBK-b26BTA"}},{"cell_type":"markdown","source":["### Model - 3"],"metadata":{"id":"8o_ZH1yo6DDR"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.metrics import classification_report\n","\n","# Initialize Extra Trees Classifier\n","model_3 = ExtraTreesClassifier(n_estimators=100, random_state=42)\n","\n","# Fit the model\n","model_3.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_et = model_3.predict(X_test)\n","\n","# Evaluate the model\n","print(classification_report(y_test, y_pred_et))"],"metadata":{"id":"x9dW516w6OS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"OmE35wWk_FnR"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","# Create a dictionary for evaluation metrics\n","evaluation_dict_et = {\n","    'Model Name': 'Model_3',\n","    'Model Type': 'Extra Trees Classifier',\n","    'Accuracy': accuracy_score(y_test, y_pred_et),\n","    'Recall': recall_score(y_test, y_pred_et, average='weighted'),  # Use 'weighted' for multiclass\n","    'Precision': precision_score(y_test, y_pred_et, average='weighted'),  # Use 'weighted' for multiclass\n","    'F1-Score': f1_score(y_test, y_pred_et, average='weighted')  # Use 'weighted' for multiclass\n","}\n","\n","# Convert the dictionary into a DataFrame\n","evaluation_df_et = pd.DataFrame(evaluation_dict_et, index=[0])\n","\n","# Output the DataFrame\n","evaluation_df_et"],"metadata":{"id":"rWNbDj9R_I2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","# Visualizing Confusion Matrix for Extra Trees Classifier\n","ConfusionMatrixDisplay.from_estimator(model_3, X_test, y_test)\n","plt.title('Confusion Matrix - Extra Trees Classifier', fontweight='bold')\n","plt.show()"],"metadata":{"id":"8rUWdp_N_MXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","import numpy as np\n","\n","# Binarize the output labels for multi-class ROC curve calculation\n","n_classes = len(np.unique(y_test))  # Adjust according to your dataset\n","y_test_binarized = label_binarize(y_test, classes=np.arange(1, n_classes + 1))\n","\n","# Predict probabilities for the ROC curve\n","y_score_et = model_3.predict_proba(X_test)\n","\n","# Initialize a ROC curve plot\n","plt.figure()\n","\n","# For each class, compute the ROC curve\n","for i in range(n_classes):\n","    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score_et[:, i])\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'Class {i + 1} ROC (AUC = {roc_auc:.2f})')\n","\n","# Plot diagonal for random guessing\n","plt.plot([0, 1], [0, 1], 'k--')\n","\n","# Add labels and title\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve for Extra Trees Classifier - Before Cross-Validation')\n","plt.legend(loc=\"lower right\")\n","\n","# Display the ROC curve plot\n","plt.show()"],"metadata":{"id":"7ls0UXC9_RDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 2. Cross- Validation"],"metadata":{"id":"9aJSPOzv_aCB"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","\n","# Perform cross-validation (cv=3 for 3-fold cross-validation)\n","cv_scores_et = cross_val_score(model_3, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n","\n","# Output the cross-validation scores\n","print(\"Cross-Validation Scores (Extra Trees):\", cv_scores_et)\n","print(\"Mean CV Accuracy (Extra Trees):\", cv_scores_et.mean())"],"metadata":{"id":"ZS3IRf-9_isk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which cross-validation optimization technique have you used and why?"],"metadata":{"id":"0LHhb2sxdRTJ"}},{"cell_type":"markdown","source":["I utilized K-Fold Cross-Validation for optimizing the Extra Trees model. By partitioning the dataset into three folds, each fold serves as a test set once, while the remaining two folds are employed for training. The cross-validation scores obtained from the Extra Trees model were [0.7938, 0.8329, 0.9756], reflecting its performance across various subsets. The mean CV accuracy of approximately 0.8674 indicates the model's effectiveness and its capability to generalize well to unseen data."],"metadata":{"id":"_U_hfYQ6dX-R"}},{"cell_type":"code","source":["# Refit the model if cross-validation gave you new hyperparameters (if tuning)\n","# Otherwise, continue using the previously fitted model\n","\n","# Predict probabilities for the ROC curve after cross-validation\n","y_score_et_cv = model_3.predict_proba(X_test)\n","\n","# Initialize a ROC curve plot\n","plt.figure()\n","\n","# For each class, compute the ROC curve after cross-validation\n","for i in range(n_classes):\n","    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score_et_cv[:, i])\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'Class {i + 1} ROC (AUC = {roc_auc:.2f})')\n","\n","# Plot diagonal for random guessing\n","plt.plot([0, 1], [0, 1], 'k--')\n","\n","# Add labels and title\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve for Extra Trees Classifier - After Cross-Validation')\n","plt.legend(loc=\"lower right\")\n","\n","# Display the ROC curve plot\n","plt.show()"],"metadata":{"id":"t4e9pIEQeh6q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Enhancements Following Cross-Validation:"],"metadata":{"id":"ODeMDSuofPQu"}},{"cell_type":"markdown","source":["After conducting cross-validation, the AUC for the Extra Trees model remained unchanged, indicating that the model's ability to distinguish between classes was consistent. This stability suggests that the model is reliable and that the cross-validation process effectively validated its performance without introducing significant variations."],"metadata":{"id":"8p4G3Z6ZfU4_"}},{"cell_type":"markdown","source":["#### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"TX294-LBfXYW"}},{"cell_type":"markdown","source":["1. Precision: Ensures that the model accurately targets the right customers, minimizing the risk of false positives and reducing unnecessary marketing efforts.\n","\n","2. Recall: Prioritizes identifying as many interested customers as possible, ensuring no potential customers are overlooked.\n","\n","3. F1 Score: Provides a balanced measure by combining precision and recall, offering a clearer view of the model’s overall performance.\n","\n","4. ROC-AUC: Evaluates the model’s ability to differentiate between interested and non-interested customers across various thresholds, giving insight into its classification effectiveness.\n","\n","5. Confusion Matrix: Highlights specific prediction errors, helping to pinpoint areas for improvement in the model's decision-making process."],"metadata":{"id":"WNGSZbS4faU8"}},{"cell_type":"markdown","source":["#### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"5FrbdOxVfwn1"}},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","# Evaluation metrics for each model\n","evaluation_dicts = []\n","\n","# Model 1 metrics\n","evaluation_dicts.append({\n","    'Model Name': 'Model 1',\n","    'Accuracy': accuracy_score(y_test, model_1_pred),  # Replace with your variable y_test, y_pred_model_1\n","    'Recall': recall_score(y_test, model_1_pred, average='weighted'),\n","    'Precision': precision_score(y_test, model_1_pred, average='weighted'),\n","    'F1-Score': f1_score(y_test, model_1_pred, average='weighted')\n","})\n","\n","# Model 2 metrics\n","evaluation_dicts.append({\n","    'Model Name': 'Model 2',\n","    'Accuracy': accuracy_score(y_test, model_2_pred),  # Replace with your variable y_test, y_pred_model_2\n","    'Recall': recall_score(y_test, model_2_pred, average='weighted'),\n","    'Precision': precision_score(y_test, model_2_pred, average='weighted'),\n","    'F1-Score': f1_score(y_test, model_2_pred, average='weighted')\n","})\n","\n","# Model 3 (Extra Trees) metrics\n","evaluation_dicts.append({\n","    'Model Name': 'Model 3',\n","    'Accuracy': accuracy_score(y_test, y_pred_et),  # Replace with your variable y_test, y_pred_model_3\n","    'Recall': recall_score(y_test, y_pred_et, average='weighted'),\n","    'Precision': precision_score(y_test, y_pred_et, average='weighted'),\n","    'F1-Score': f1_score(y_test, y_pred_et, average='weighted')\n","})\n","\n","# Create DataFrame\n","evaluation_df = pd.DataFrame(evaluation_dicts)\n","\n","# Melt the DataFrame for plotting\n","df_melted = evaluation_df.melt(id_vars=['Model Name'], var_name='Metric', value_name='Value')\n","\n","# Plotting\n","plt.figure(figsize=(14, 6))\n","sns.barplot(x='Model Name', y='Value', hue='Metric', data=df_melted, palette='Blues')\n","\n","# Add a horizontal line at 0.90\n","plt.axhline(y=0.90, color='red', linestyle='--', linewidth=1)\n","\n","# Titles and labels\n","plt.title('Model Performance Metrics')\n","plt.xlabel('Models')\n","plt.ylabel('Value')\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.legend(title='Metric', bbox_to_anchor=(1, 1), loc='upper left')\n","\n","# Adjust layout\n","plt.tight_layout()\n","\n","# Show plot\n","plt.show()"],"metadata":{"id":"5A3I91qFf3FB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Selected Model: Model - 3 (Extra Trees)"],"metadata":{"id":"VnXG42wdf7JU"}},{"cell_type":"markdown","source":["I have chosen the Extra Trees model as my final selection for predicting the CSAT score due to its superior accuracy, recall, precision, and F1-score compared to all other models evaluated.\n","\n","Accuracy: The Extra Trees model achieved an impressive accuracy of 0.954624, making it the highest-performing model among those evaluated. Recall: It exhibits a strong recall of 0.954624, ensuring that it captures a significant portion of relevant instances. Precision: With a precision score of 0.956735, the model effectively targets the most pertinent cases. F1-Score: The Extra Trees model also boasts an F1-Score of 0.953071, indicating a solid balance between precision and recall.\n","\n","Overall, the Extra Trees model provides an exceptional combination of high accuracy, recall, precision, and F1-Score, making it the most effective model for predicting CSAT scores."],"metadata":{"id":"lXAUdCKxf95y"}},{"cell_type":"markdown","source":["#### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"LceyrE-VgARE"}},{"cell_type":"code","source":["feature_df = pd.DataFrame({'feature_name ': X_train.columns, 'feature_importance': model_3.feature_importances_})\n"],"metadata":{"id":"QbrWNn31gCTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_df.sort_values(by='feature_importance', ascending=False, inplace=True)\n","feature_df"],"metadata":{"id":"L-m31VWSgI_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating feature importance DataFrame\n","feature_df = pd.DataFrame({\n","    'feature_name': X_train.columns,\n","    'feature_importance': model_3.feature_importances_\n","})\n","\n","# Sort the DataFrame by feature importance\n","feature_df = feature_df.sort_values(by='feature_importance', ascending=False)\n","\n","# Adjust the figure size for 126 features\n","plt.figure(figsize=(15, 30))  # Adjusted height for 126 features\n","\n","# Create the horizontal bar plot\n","plt.barh(feature_df['feature_name'], feature_df['feature_importance'], color='skyblue')\n","\n","# Labeling and title\n","plt.xlabel('Feature Importance', fontsize=14)\n","plt.title('Feature Importance from Extra Trees Model', fontsize=16)\n","plt.gca().invert_yaxis()  # Most important feature on top\n","\n","# Adjust margins to reduce the whitespace on the left\n","plt.subplots_adjust(left=0.3)  # Increase left margin to reduce whitespace\n","\n","# Improving the layout\n","plt.tight_layout()\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"YMZ4_JT6gNJ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"3wrZIxSxgQ9C"}},{"cell_type":"markdown","source":["1. Issue Responded At: The feature \"issue_responded_at\" has a high importance score, indicating that it is a strong predictor of the CSAT (Customer Satisfaction) score. This suggests that the timeliness of addressing customer issues significantly impacts their satisfaction.\n","\n","2. Survey Response Date: The feature \"survey_response_date\" also plays a crucial role in predicting the CSAT score, suggesting that customer satisfaction is closely linked to when the feedback is provided.\n","\n","3. Order ID: Although \"Order_id\" may not directly influence customer satisfaction, its presence helps track and associate specific customer orders with their corresponding feedback, which could be useful for analysis and identifying trends across different orders."],"metadata":{"id":"k2fNjfHOgTjj"}},{"cell_type":"markdown","source":["# **8. Future Work** **(Optional)**"],"metadata":{"id":"z0wYedBahUzg"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process."],"metadata":{"id":"h0A3bmazhjgD"}},{"cell_type":"code","source":["!pip install joblib"],"metadata":{"id":"ZTu6ylAWh5zw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fWA_q_KOia-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","\n","# Define the path where you want to save the model/content/drive/MyDrive/Almabetter/Almabetter projects\n","path = \"/content/drive/MyDrive/\"\n","\n","# Save the ExtraTrees model (assuming 'model_3' is your trained model)\n","joblib.dump(model_3, path + \"/et_model.pkl\")\n","\n","print(f\"Model saved successfully at {path}/et_model.pkl\")"],"metadata":{"id":"NKKqXQuwh9X_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check."],"metadata":{"id":"FD6qjWOGjFaw"}},{"cell_type":"code","source":["# Define the path where the model is saved\n","path = \"/content/drive/MyDrive//et_model.pkl\"\n","\n","# Load the saved model\n","load_model = joblib.load(path)"],"metadata":{"id":"C3ZpYwnljH7X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","# Use the loaded model to predict on the test set\n","load_pred = load_model.predict(X_test)\n","\n","# Calculate and print the metrics\n","print(f'Accuracy: {accuracy_score(y_test, load_pred)}')\n","print(f'Recall: {recall_score(y_test, load_pred, average=\"weighted\")}')\n","print(f'Precision: {precision_score(y_test, load_pred, average=\"weighted\")}')\n","print(f'F1-Score: {f1_score(y_test, load_pred, average=\"weighted\")}')"],"metadata":{"id":"LxrlDycmjRD-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!**"],"metadata":{"id":"7ykiPzpejd_o"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["Write the conclusion here."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["In this classification project, I have successfully implemented various machine learning models to predict customer satisfaction (CSAT) scores. After evaluating multiple algorithms, I have selected the Extra Trees model as the final choice due to its exceptional performance metrics, including high accuracy, recall, precision, and F1-score.\n","\n","The analysis included assessing feature importance, revealing insights into which features significantly influence CSAT predictions. The project demonstrated the effectiveness of model selection and feature analysis in enhancing predictive capabilities. Overall, this classification project not only provided valuable predictions for customer satisfaction but also highlighted the importance of careful model evaluation and feature engineering in achieving robust outcomes."],"metadata":{"id":"3jg2qQcOjjkD"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your EDA Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}